{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae42e62d-52d5-41e3-bb1a-0c7ba65c3ab7",
   "metadata": {},
   "source": [
    "# Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114df7ea-c3fa-47d7-bafc-9fe76df81d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd87a4ee-34c6-40c8-be95-b0bfa5f56e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from torchsummary import summary\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import os, time, math, copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(precision=8, linewidth=50000)\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936610fc-b826-4105-b8f5-aca27e1f55cd",
   "metadata": {},
   "source": [
    "# Print Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c80e2d-6adf-4fee-832d-ca9be8808d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACK\t= '\\033[30m'\n",
    "RED\t\t= '\\033[31m'\n",
    "GREEN\t= '\\033[32m'\n",
    "YELLOW\t= '\\033[33m'\n",
    "BLUE\t= '\\033[34m'\n",
    "MAGENTA\t= '\\033[35m'\n",
    "CYAN\t= '\\033[36m'\n",
    "RESET\t= '\\033[0m'\n",
    "SEL\t\t= '\\033[7m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07042d8d-61a1-4895-b51c-ef1a9584334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class\tfxp:\n",
    "\tdef\t__init__(self, bIn, iBWF):\n",
    "\t\tself.iFullBW\t= len(bIn)\n",
    "\t\tself.iIntgBW\t= self.iFullBW - iBWF\n",
    "\t\tself.bSign\t\t= bIn[0]\n",
    "\t\tself.bIntg\t\t= bIn[:self.iIntgBW]\n",
    "\t\tself.bFrac\t\t= bIn[self.iIntgBW:]\n",
    "\t\tself.fFull\t\t= 0\n",
    "\t\ttry:\n",
    "\t\t\tfor idx, bit in enumerate(bIn):\n",
    "\t\t\t\tif\tidx == 0:\n",
    "\t\t\t\t\tself.fFull = self.fFull + int(bit,2) * -pow(2, self.iIntgBW - 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.fFull = self.fFull + int(bit,2) * pow(2, self.iIntgBW - 1 - idx)\n",
    "\t\texcept:\n",
    "\t\t\tprint(bIn)\n",
    "\t\tself.dispFull\t= RED + self.bIntg + BLUE + self.bFrac + RESET\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a5be79-0b57-4d5d-9229-dcfe6900e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class\tflp2fix:\n",
    "\tdef\t__init__(self, fIn, iBW, iBWF):\n",
    "\t\tself.fMin\t\t= - 2 ** (iBW - iBWF - 1)\n",
    "\t\tself.fMax\t\t= (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "\t\tself.fResol\t\t= 2 ** -iBWF\n",
    "\t\tif fIn < self.fMin or fIn > self.fMax:\n",
    "\t\t\tprint(f'({fIn}): Out of input range ({self.fMax}/{self.fMin}) during flp -> fix converting ')\n",
    "\t\tself.iBW\t\t= iBW\n",
    "\t\tself.iBWI\t\t= iBW - iBWF\n",
    "\t\tself.iBWF\t\t= iBWF\n",
    "\n",
    "\t\tself.iFLP2INT\t= abs(int(fIn * 2 ** iBWF))\n",
    "\t\tif fIn < 0:\n",
    "\t\t\tself.iFLP2INT = 2 ** (iBW-1) - self.iFLP2INT\n",
    "\n",
    "\t\tif fIn >= 0:\n",
    "\t\t\tself.bFull = bin(self.iFLP2INT)[2:].rjust(iBW, '0')\n",
    "\t\telse:\n",
    "\t\t\tself.bFull = '1'+bin(self.iFLP2INT)[2:].rjust(iBW-1, '0')\n",
    "\t\t\tif len(self.bFull) > iBW:\n",
    "\t\t\t\tself.bFull = '0' * iBW\n",
    "\n",
    "\t\tself.cssFxp\t\t= fxp(self.bFull, self.iBWF)\n",
    "\t\tself.bSign\t\t= self.cssFxp.bSign\n",
    "\t\tself.bIntg\t\t= self.cssFxp.bIntg\n",
    "\t\tself.bFrac\t\t= self.cssFxp.bFrac\n",
    "\t\tself.fFull\t\t= self.cssFxp.fFull\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75346b45-fad2-4643-a086-9f79c75048ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def\tflp2fixTensor(fIn, iBW, iBWF):\n",
    "\tfMin = - 2 ** (iBW - iBWF - 1)\n",
    "\tfMax = (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "\tfList = []\n",
    "\tfor aTensor in fIn.view(-1):\n",
    "\t\tfList.append(flp2fix(aTensor, iBW, iBWF).fFull)\n",
    "\treturn torch.tensor(fList).view(fIn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd276c-3039-4782-b9cc-3a025e5f2e08",
   "metadata": {},
   "source": [
    "# User Define Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550c26cd-69ef-492b-9516-3631b6507c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '~/dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81da7c0-fc83-4be2-8e6d-44c0c034bc40",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "644a1959-2599-463e-990f-c4266e73ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch for MNIST dataset')\n",
    "parser.add_argument('--device', type=str, default='cpu', help='Device')\n",
    "parser.add_argument('--shuffle', action='store_true', default=False, help='enables data shuffle')\n",
    "parser.add_argument('--dataset', type=str, default='mnist', help='training dataset')\n",
    "parser.add_argument('--data_path', type=str, default=data_path, help='path to MNIST')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='optimizer')\n",
    "parser.add_argument('--loss_func', type=str, default='cel', help='optimizer')\n",
    "parser.add_argument('--quant_opt', type=str, default='asym', help='Type of Quantization')\n",
    "parser.add_argument('--full_bits', type=int, default=16, help='Number of Quantization Bits')\n",
    "parser.add_argument('--frac_bits', type=int, default=8, help='Number of Quantization Bits')\n",
    "parser.add_argument('--pretrained', type=bool, default=True, help='Pretrained Model')\n",
    "parser.add_argument('--act_quant', type=bool, default=False, help='Activation Quantization')\n",
    "parser.add_argument('--disp', type=bool, default=False, help='Display Model Information')\n",
    "\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38083626-e399-49d1-8528-7ad07d2f57e5",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d593f93-d5ae-42f8-931e-4b6ab0889bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/jungeun/dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.0%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "75.1%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "97.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/jungeun/dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/jungeun/dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/jungeun/dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/jungeun/dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/jungeun/dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/jungeun/dataset/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.device == 'cuda' else {}\n",
    "transforms = transforms.Compose([transforms.Resize((32,32)),transforms.ToTensor()])\n",
    "if args.dataset == 'mnist':\n",
    "\ttrain_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=True,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=transforms\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)\n",
    "\n",
    "\ttest_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=False,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=transforms\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac15596-d02e-4dfa-aed6-12d0af91c487",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee464dbd-aac4-4d82-8787-270f8cd3ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet5(nn.Module):\n",
    "    def __init__(self): #layer sequential define\n",
    "        super(Lenet5, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5*5 square convolution\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Conv2d1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5, stride = 1)\n",
    "        self.Conv2d2 = nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = 5, stride = 1)\n",
    "        self.Conv2d3 = nn.Conv2d(in_channels = 16, out_channels = 120, kernel_size = 5, stride = 1)\n",
    "        self.AvgPool2d = nn.AvgPool2d(kernel_size = 2)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.Linear1 = nn.Linear(120, 84)\n",
    "        self.Linear2 = nn.Linear(84, 10)\n",
    "    def forward(self, x) :\n",
    "        x = self.Conv2d1(x)\n",
    "        x = self.Tanh(x)\n",
    "        x = self.AvgPool2d(x)\n",
    "        x = self.Conv2d2(x)\n",
    "        x = self.Tanh(x)\n",
    "        x = self.AvgPool2d(x)\n",
    "        x = self.Conv2d3(x)\n",
    "        x = self.Tanh(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.Linear1(x)\n",
    "        x = self.Tanh(x)\n",
    "        x = self.Linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87b94f73-bd08-4b09-937c-b0788fba1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genOptimizer(model, args):\n",
    "\tif args.optimizer == 'sgd':\n",
    "\t\toptimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\tif args.optimizer == 'adam':\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f3af82-03a9-4268-a864-87e3afaee89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genLossFunc(args):\n",
    "\tif args.loss_func == 'cel':\n",
    "\t\tloss_func = nn.CrossEntropyLoss()\n",
    "\treturn loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fb2e20e-9352-4066-8127-1a4118f076d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epoch, args):\n",
    "\tmodel.train()\n",
    "\tloss_func = genLossFunc(args)\n",
    "\toptimizer = genOptimizer(model, args)\n",
    "\tmax_batch_index = int(np.floor(len(train_loader.dataset)/args.batch_size)) #batch 번호 ..?\n",
    "\trunning_loss = 0\n",
    "\tfor batch_index, (image, label) in enumerate(tq(train_loader, desc='Train', leave=False)):\n",
    "\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\tpred = model(image) #pred = model\n",
    "\t\tloss = loss_func(pred, label) # model 이용해서 loss 구함\n",
    "\t\trunning_loss += loss.item()#*image.size(0)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\tprint(f'Epoch {epoch+1:<3d}: Avg. Loss: {running_loss/len(train_loader.dataset):.4f}', end = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd446a25-819f-4b37-989d-50a0353f6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, args):\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss_func = genLossFunc(args)\n",
    "\t\tloss, correct = 0, 0\n",
    "# \t\tfor batch_index, (image, label) in enumerate(tq(test_loader, desc='Test', leave=False)):\n",
    "\t\tfor batch_index, (image, label) in enumerate(test_loader):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tpred = model(image)\n",
    "\t\t\tloss += loss_func(pred, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (pred.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_loader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c653a9bd-781a-47a9-bdcd-32f43bd1f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model):\n",
    "\tfor epoch in range(args.epochs):\n",
    "\t\ttrain(train_loader, model, epoch, args)\n",
    "\t\ttest(test_loader, model, args)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "485b1021-2707-4973-9c61-2a0273e71051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  : Avg. Loss: 0.0048\tAccuracy: 9582/10000 (95.8%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  : Avg. Loss: 0.0016\tAccuracy: 9735/10000 (97.3%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  : Avg. Loss: 0.0011\tAccuracy: 9783/10000 (97.8%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4  : Avg. Loss: 0.0008\tAccuracy: 9793/10000 (97.9%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5  : Avg. Loss: 0.0006\tAccuracy: 9803/10000 (98.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6  : Avg. Loss: 0.0005\tAccuracy: 9804/10000 (98.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7  : Avg. Loss: 0.0004\tAccuracy: 9793/10000 (97.9%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8  : Avg. Loss: 0.0003\tAccuracy: 9807/10000 (98.1%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9  : Avg. Loss: 0.0003\tAccuracy: 9823/10000 (98.2%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 : Avg. Loss: 0.0003\tAccuracy: 9821/10000 (98.2%)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = main(Lenet5().to(args.device))\n",
    "torch.save(model.state_dict(), 'PTQ.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "350ed278-1280-4b87-9c3e-df22702ae861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenet5(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (Conv2d1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (Conv2d2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (Conv2d3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (AvgPool2d): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (Tanh): Tanh()\n",
      "  (Linear1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (Linear2): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f59dbb-5579-4196-bc32-c579a258bdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             156\n",
      "              Tanh-2            [-1, 6, 28, 28]               0\n",
      "         AvgPool2d-3            [-1, 6, 14, 14]               0\n",
      "            Conv2d-4           [-1, 16, 10, 10]           2,416\n",
      "              Tanh-5           [-1, 16, 10, 10]               0\n",
      "         AvgPool2d-6             [-1, 16, 5, 5]               0\n",
      "            Conv2d-7            [-1, 120, 1, 1]          48,120\n",
      "              Tanh-8            [-1, 120, 1, 1]               0\n",
      "            Linear-9                   [-1, 84]          10,164\n",
      "             Tanh-10                   [-1, 84]               0\n",
      "           Linear-11                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,input_size=(1,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a012bf2a-6e27-4f18-9060-e95acc65299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2fix(model, args):\n",
    "\tfor name, _ in model.named_parameters():\n",
    "\t\texec(f'model.{name}.data = flp2fixTensor(model.{name}.data, {args.full_bits}, {args.frac_bits})')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b2381db-f666-42a7-a1c5-14783fd70c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantFixForward(model, x, args):\n",
    "    cmodel = copy.deepcopy(model).to(args.device)\n",
    "    with torch.no_grad():\n",
    "        act0 = cmodel.AvgPool2d(cmodel.Tanh(cmodel.Conv2d1(x))) #activation\n",
    "        act0 = flp2fixTensor(act0, args.full_bits, args.frac_bits)\n",
    "\n",
    "        act1 = cmodel.AvgPool2d(cmodel.Tanh(cmodel.Conv2d2(act0)))\n",
    "        act1 = flp2fixTensor(act1, args.full_bits, args.frac_bits)\n",
    "\n",
    "        act2 = cmodel.Tanh(cmodel.Conv2d3(act1))\n",
    "        act2 = flp2fixTensor(act2, args.full_bits, args.frac_bits)\n",
    "\n",
    "        act3 = cmodel.flatten(act2)\n",
    "        act3 = flp2fixTensor(act3, args.full_bits, args.frac_bits)\n",
    "        \n",
    "        act4 = cmodel.Tanh(cmodel.Linear1(act3))\n",
    "        act4 = flp2fixTensor(act4, args.full_bits, args.frac_bits)\n",
    "        \n",
    "        act5 = cmodel.Linear2(act4)\n",
    "        act5 = flp2fixTensor(act5, args.full_bits, args.frac_bits)\n",
    "\n",
    "    return cmodel, act0, act1, act2, act3, act4, act5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6ce915d-0d0f-47b9-9773-8083124152d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQuant(model, test_loader, args):\n",
    "    qmodel = copy.deepcopy(model).to(args.device)\n",
    "    qmodel = model2fix(qmodel, args)\n",
    "    qmodel.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_func = genLossFunc(args)\n",
    "        loss, correct = 0, 0\n",
    "        for batch_index, (image, label) in enumerate(test_loader):\n",
    "            image, label = image.to(args.device), label.to(args.device)\n",
    "            qmodel, act0, act1, act2, act3, act4, act5  = quantFixForward(qmodel, image, args)\n",
    "            y = act5\n",
    "            loss += loss_func(y, label).item()#*image.size(0)\n",
    "            correct += (y.argmax(1) == label).type(torch.int).sum().item()\n",
    "    correct_rate = 100 * correct / len(test_loader.dataset)\n",
    "    print(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%) Loss: {loss/len(test_loader.dataset):.2f}')\n",
    "    return qmodel, act0, act1, act2, act3, act4, act5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ee43f0-f770-41fe-9c84-504ed808ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9817/10000 (98.2%) Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "qmodel, act0, act1, act2, act3, act4, act5 = testQuant(model, test_loader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cec46f8-2168-4218-80c9-a505513c2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractParams(model, args):\n",
    "    for key in model.state_dict().keys():\n",
    "        layer_name = key.split('.')[0]\n",
    "        param_type = 'w' if 'weight' in key else 'b'\n",
    "        for idx, params in enumerate(eval(f'qmodel.{key}.data')):\n",
    "            with open(f'mif/{layer_name}_{param_type}_{idx}.mif', 'w') as fh:\n",
    "                if param_type == 'w':\n",
    "                    if params.dim() == 1 :\n",
    "                        #print(f'param dim is {params.dim()}')\n",
    "                        for idx, param in enumerate(params):\n",
    "                            bin_param = flp2fix(param, args.full_bits, args.frac_bits).bFull\n",
    "                            fh.write(bin_param + ('\\n','')[idx == len(params)-1])\n",
    "                    elif params.dim() == 3 :\n",
    "                        #print(f'param dim is {params.dim()}')\n",
    "                        for idx, dim1 in enumerate(params):\n",
    "                            for idx, dim2 in enumerate(dim1):\n",
    "                                for idx, param in enumerate(dim2) :\n",
    "                                    bin_param = flp2fix(param, args.full_bits, args.frac_bits).bFull\n",
    "                                    fh.write(bin_param + ('\\n','')[idx == len(params)-1])\n",
    "                else:\n",
    "                    bin_param = flp2fix(params, args.full_bits, args.frac_bits).bFull\n",
    "                    fh.write(bin_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c8a31dc-7015-4e44-9cd8-be2fd624a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genInputVector(test_loader, args):\n",
    "    out_path = './vec'\n",
    "    os.system(f'rm -rf {out_path};mkdir -p {out_path}')\n",
    "    with open(f'{out_path}/labels.vec', 'w') as fh_labels:\n",
    "        with open(f'{out_path}/images.vec', 'w') as fh_images:\n",
    "            for batch_index, (images, labels) in enumerate(test_loader):\n",
    "                for (image, label) in zip(images, labels):\n",
    "                    bin_label = flp2fix(label, args.full_bits, 0).bFull\n",
    "                    fh_labels.write(bin_label+'\\n')\n",
    "                    for pixel in image.view(-1):\n",
    "                        bin_pixel = flp2fix(pixel, args.full_bits, args.frac_bits).bFull\n",
    "                        fh_images.write(bin_pixel+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ef70461-d364-4a85-92cd-0c39f0675132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not args.pretrained:\n",
    "extractParams(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d9791cf-47df-42eb-bcfd-f1e235598ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "genInputVector(test_loader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fede9d53-d71e-47dd-9440-c62d6c83b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(\troot=args.data_path,\n",
    "\t\t\t\t\t\t\ttrain=False,\n",
    "\t\t\t\t\t\t\tdownload=True,\n",
    "\t\t\t\t\t\t\ttransform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "607e2d07-f8ca-4820-b046-3781c9062383",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_index, (image, label) in enumerate(test_loader):\n",
    "           image, label = image.to(args.device), label.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87d71b4b-f97e-44af-b8cb-051ec5b1ee81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99e545a3-584f-4dc7-b297-fbcbaa6641d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b1dbbf8-f3fc-4da4-83bc-86f7543101be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fe6e19a-2b4c-4cac-8ea9-763de12e85d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 32])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_input = image\n",
    "a_input.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43b4ec-c7af-48cf-8d1c-4c03a14f0949",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afdc0c-7bb9-42f4-a23b-e0faf6cce086",
   "metadata": {},
   "source": [
    "### conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4569ce2a-873f-4c72-af87-509498c7c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(X, filters,bias, stride=1, pad=0):\n",
    "    n, c, h, w = X.shape # 1, 1, 32, 32\n",
    "    n_f, _, filter_h, filter_w = filters.shape\n",
    "    \n",
    "    out_h = (h+2*pad-filter_h)//stride + 1\n",
    "    out_w = (w+2*pad-filter_w)//stride + 1\n",
    "    # add padding to height and width.\n",
    "    in_X = F.pad(X,(0,0,0,0,pad,pad,pad,pad),\"constant\", 0)\n",
    "    out  = torch.zeros((n, n_f, out_h, out_w))\n",
    "    \n",
    "    for i in range(n): # for each image.\n",
    "        for c in range(n_f): # for each channel.\n",
    "            for h in range(out_h): # slide the filter vertically.\n",
    "                h_start = h * stride\n",
    "                h_end = h_start + filter_h\n",
    "                for w in range(out_w): # slide the filter horizontally.\n",
    "                    w_start = w * stride\n",
    "                    w_end = w_start + filter_w\n",
    "                    # Element-wise multiplication.\n",
    "                    out[i, c, h, w] = torch.sum(in_X[i,:,h_start:h_end,w_start:w_end]*filters[c])+bias[c]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d097de5-3122-4451-a29e-9ecd2b8e30eb",
   "metadata": {},
   "source": [
    "### tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ee8d591-bbf5-4fcf-b206-bb8308e84983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(X):\n",
    "    n, c, h, w = X.shape\n",
    "    \n",
    "    out = torch.zeros(n,c,h,w)\n",
    "    \n",
    "    for i in range(n): #for each image\n",
    "        for ch in range(c) : #for each channel\n",
    "            for o_h in range(h) : #for each height\n",
    "                for o_w in range(w) : #for each width\n",
    "                    x = X[i, ch, o_h, o_w]\n",
    "                    out[i, ch, o_h, o_w] = (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b514de-2220-4e48-8c38-7eee5d2eba6f",
   "metadata": {},
   "source": [
    "### avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a4628c8-42b1-4dcb-838d-20ed940f4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgpool2d(X,kernel_size,stride,pad=0):\n",
    "    n, c, h, w = X.shape\n",
    "    ker_w, ker_h = kernel_size\n",
    "    \n",
    "    out_h = (h + 2*pad - ker_h)//stride + 1\n",
    "    out_w = (w + 2*pad - ker_w)//stride + 1\n",
    "    \n",
    "    out = torch.zeros(n,c,out_h,out_w)\n",
    "    for i in range(n) : #for each image\n",
    "        for ch in range(c) : #for each channel \n",
    "             for h in range(out_h) :\n",
    "                    h_start = h * stride\n",
    "                    h_end = h_start + ker_w\n",
    "                    for w in range(out_w):\n",
    "                        w_start = w * stride\n",
    "                        w_end = w_start + ker_w\n",
    "                        #element average\n",
    "                        out[i, ch, h, w] = torch.mean(X[i,ch,h_start:h_end,w_start:w_end])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f5e21-3675-4a78-9f94-4fbf4e1013a3",
   "metadata": {},
   "source": [
    "### linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f176c-8735-4dcb-a763-6e685d280851",
   "metadata": {},
   "source": [
    "## checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3fa2d71b-b6d4-466e-b840-a598f7a57742",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters1 = qmodel.Conv2d1.weight\n",
    "filters2 = qmodel.Conv2d2.weight\n",
    "filters3 = qmodel.Conv2d3.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ecbe49f-47f5-44d8-a408-df727d92c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias1 = qmodel.Conv2d1.bias\n",
    "bias2 = qmodel.Conv2d2.bias\n",
    "bias3 = qmodel.Conv2d3.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84792791-321a-4fc9-931e-59a8d156c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_filter1 = qmodel.Linear1.weight\n",
    "linear_filter2 = qmodel.Linear2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f048caa-5905-4324-9d06-f8056b5806c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_bias1 = qmodel.Linear1.bias\n",
    "linear_bias2 = qmodel.Linear2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea6a0c8e-c3a2-430a-8311-d18c1877101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84, 120])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_filter1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87dd94e7-f94e-4821-b7e7-a54144b5161b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 84])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_filter2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e07e1-d32a-49cf-9c7d-a65d4e66c61e",
   "metadata": {},
   "source": [
    "### act0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd151a5c-362d-4b25-8af9-0fe4d1ec9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1 = conv(a_input,filters1,bias1,stride=1,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ceb262ca-a1f5-4539-b0dc-e5362b34b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_tanh1 = tanh(layer_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5aca4a62-febd-4d03-b1d3-2c6db27fc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_avgpool1 = avgpool2d(layer_tanh1,(2,2),2,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9274cd8-39f2-46e4-bb16-0d8abc208992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 14, 14])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_avgpool1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84b4e9e4-b3ed-42b3-8775-bfd6cadbcc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 14, 14])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa6f87ac-43ba-48e0-8e76-077b7743b822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different value is : 0/18816\n"
     ]
    }
   ],
   "source": [
    "fix_act0 = torch.zeros(16,6,14,14)\n",
    "\n",
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(6) :\n",
    "        for c in range(14) :\n",
    "            for d in range(14) :\n",
    "                fix_act0[a][b][c][d] = flp2fix(layer_avgpool1[a][b][c][d],args.full_bits,args.frac_bits).fFull\n",
    "                if (torch.equal(fix_act0[a][b][c][d],act0[a][b][c][d])==False):\n",
    "                    num += 1\n",
    "                #bin_conv1[a][b][c][d] = flp2fix(fix_conv1[a][b][c][d],args.full_bits,args.frac_bits).bFull\n",
    "                total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b48be63e-b475-43a3-96c8-1b47ab42a723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fix_act0,act0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19a199-0b9a-488d-8b00-97e49ed5729f",
   "metadata": {},
   "source": [
    "### act1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b97ec681-83b9-4585-a937-862ae90a6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv2 = conv(fix_act0,filters2,bias2,stride=1,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a707f846-356f-4d1d-82b2-65b877373350",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_tanh2 = tanh(layer_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "caeaa0b5-8da9-44dc-a72d-fdd40a2176e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_avgpool2 = avgpool2d(layer_tanh2,(2,2),2,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca2fdbc4-b15c-4002-8175-0a8693ee5857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 5, 5])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_avgpool2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "542f6201-c8c4-44ea-be0a-ab52be8a1e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 5, 5])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "41996925-65cc-4bf8-93ce-f08c57b3ca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.99609375\n",
      "1.0\n",
      "0.99609375\n",
      "-1.0\n",
      "-0.99609375\n",
      "number of different value is : 3/6400\n"
     ]
    }
   ],
   "source": [
    "fix_act1 = torch.zeros(16,16,5,5)\n",
    "\n",
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(16) :\n",
    "        for c in range(5) :\n",
    "            for d in range(5) :\n",
    "                fix_act1[a][b][c][d] = flp2fix(layer_avgpool2[a][b][c][d],args.full_bits,args.frac_bits).fFull\n",
    "                if (torch.equal(fix_act1[a][b][c][d],act1[a][b][c][d])==False):\n",
    "                    num += 1\n",
    "                    print(fix_act1[a][b][c][d].item())\n",
    "                    print(act1[a][b][c][d].item())\n",
    "                total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dfb65f3d-5088-4223-816f-9da415725f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fix_act1,act1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9f3cd71-8a41-49ba-846b-d1755d7cabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_layer_conv2 = qmodel.Conv2d2(fix_act0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "625935d2-9f7c-4846-9f0c-dfb88029479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(layer_conv2,qmodel_layer_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b037533d-31a6-444a-9523-135d9709b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_layer_tanh2 = qmodel.Tanh(qmodel_layer_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72df75c8-ba80-44fa-bffd-b513ab78ea94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(layer_tanh2,qmodel_layer_tanh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9cc7bd2c-c64b-45b1-af66-12583c1d742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different value is : 15115/25600\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(16) :\n",
    "        for c in range(10) :\n",
    "            for d in range(10) :\n",
    "                if (torch.equal(layer_tanh2[a][b][c][d],qmodel_layer_tanh2[a][b][c][d])==False):\n",
    "                    num += 1\n",
    "                total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9fa28b7-1dd1-4c5b-91af-a54f2e9ffbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_layer_avgpool2 = qmodel.AvgPool2d(qmodel_layer_tanh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e2a298fd-76ea-4195-9cb3-48c9dd63245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_fix_act1 = torch.zeros(16,16,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51655bfd-fdca-4724-b088-713dd50f970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different value is : 3/6400\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(16) :\n",
    "        for c in range(5) :\n",
    "            for d in range(5) :\n",
    "                qmodel_fix_act1[a][b][c][d] = flp2fix(qmodel_layer_avgpool2[a][b][c][d],args.full_bits,args.frac_bits).fFull\n",
    "                if (torch.equal(fix_act1[a][b][c][d],qmodel_fix_act1[a][b][c][d])==False):\n",
    "                    num += 1                \n",
    "                total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94aef8-9ecd-4cc3-996e-b03fba24bf34",
   "metadata": {},
   "source": [
    "### act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7882e74-d68c-424b-b5d0-2e9e5533f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv3 = conv(fix_act1,filters3,bias3,stride=1,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8c7b56d-c09e-4235-a6bb-3e18b05e09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_tanh1 = tanh(layer_conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c8efe4f6-0613-4e85-9011-4902eac029a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_tanh1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "358f7e44-72d7-4d10-9258-28ceee56962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120, 1, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecd8cdd0-94d1-456a-8467-2dd41cb3a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different value is : 59/1920\n"
     ]
    }
   ],
   "source": [
    "fix_act2 = torch.zeros(16,120,1,1)\n",
    "\n",
    "total = 0\n",
    "num = 0\n",
    "\n",
    "for a in range(16) :\n",
    "    for b in range(120) :\n",
    "        for c in range(1) :\n",
    "            for d in range(1) :\n",
    "                fix_act2[a][b][c][d] = flp2fix(layer_tanh1[a][b][c][d],args.full_bits,args.frac_bits).fFull\n",
    "                if (torch.equal(fix_act2[a][b][c][d],act2[a][b][c][d])==False):\n",
    "                    num += 1\n",
    "                total += 1\n",
    "\n",
    "print(\"number of different value is : {num}/{total}\".format(num=num,total=total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b11dca20-77d5-4da9-ab99-fef4727eb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_layer_conv3 = qmodel.Conv2d3(fix_act1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca4a895b-f115-45fd-85ba-7ec6dda741e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(layer_conv3,qmodel_layer_conv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b08aaf-1fa8-4574-8509-b789b65b4ac6",
   "metadata": {},
   "source": [
    "### flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02d9e741-b9cf-42e6-b6d8-7066813001db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 84])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5df066cd-2f12-4de3-8676-faba7b0d134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "act3 = fix_act2.view(16,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4018265-48b0-4866-b121-4845d343b248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5e60c078-1496-4cc4-99aa-b5363fbc09af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84, 120])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_filter1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "568bcaba-f4b1-42c2-ad62-4acc0683c3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 120])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eca09608-9ff9-44bb-bb33-4a38d6dabc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n"
     ]
    }
   ],
   "source": [
    "for i in range(16) :\n",
    "    print(act3[i].shape)\n",
    "    #act3[i] = torch.matmul(qmodel.flatten(act3[i]),linear_filter1 + linear_bias1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c60ed-5da7-433b-925e-60971dfe9b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
